{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Neural Networks, SGD, and Back Propagation \n",
    "***\n",
    "\n",
    "**Name**: \n",
    "\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday March 23rd**. Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.  Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI-4622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy).\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:10:09.992631Z",
     "start_time": "2018-03-13T09:10:08.906587Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle, gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Problem 1 - Single-Layer and Multilayer Perceptron Learning \n",
    "***\n",
    "\n",
    "**Part A**: Consider learning the following concepts with either a single-layer or multilayer perceptron where all hidden and output neurons utilize indicator activation functions.  For each of the following concepts, state whether the concept can be learned by a single-layer perceptron.  **Briefly** justify your response: \n",
    "\n",
    "i. $~ \\texttt{ NOT } x_1$ \n",
    "\n",
    "ii. $~~x_1 \\texttt{ NOR } x_2$ \n",
    "\n",
    "iii. $~~x_1 \\texttt{ XNOR } x_2$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "$$i: \\text{NOT}x_! \\text{ is 1 demension, when }x_1 \\text{ is 0, then it's 1, when} x_2 \\text{ is 1, then it's 0. It's linear separable}$$\n",
    "\n",
    "$$ii: \\text{consider }x_1 = 0 1 0 1, x_2 = 0 0 1 1, XOR = 1000. \\text{And consider 2 demention graph, }x_! \\text{ is horizontal axis, and }x_2 \\text{ is verticle axis}$$\n",
    "$$\\text{ when }x_1 \\text{ and }x_2 \\text{both are 0, then the value is 1, and when } (x_1,x_2) = (1,0),\\text{the value is 0} \\text{ it is still linear separable}$$\n",
    "\n",
    "$$iii: x_1=0101, x_2=0011, XNOR=1001, (x_!,x_2)=(0,0) \\text{ the value is 0}, (x_1,x_2)=(1,0) \\text{the value is 0}$$\n",
    "$$(x_1,x_2)=(0,1) \\text{ the value is 0}, (x_1,x_2)=(1,1) \\text{ the value is 1}$$\n",
    "$$\\text{if you put them in the 2 demention graph, it's not linear separable}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Determine an architecture and specific values of the weights and biases in a single-layer or multilayer perceptron with indicator activation functions that can learn $x_1 \\texttt{ XNOR } x_2$. Describe your architecture and state your weight matrices and bias vectors in Markdown below. Then demonstrate that your solution is correct by implementing forward propagation for your network in Python and showing that it correctly produces the correct boolean output values for each of the four possible combinations of $x_1$ and $x_2$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "$$x_1=0101$$ \n",
    "$$x_2=0011$$\n",
    "$$\\texttt{XNOR} = (x_1 \\texttt{ AND } X_2) \\texttt{ XOR } (x_1 \\texttt{ NOR } x_2)=1001$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\texttt{ AND: } W_1^{1}= \\left[ {\\begin{array}{cc}1&1 \\end{array}} \\right], b_1^{1}=[-1]$$\n",
    "$$\\texttt{ NOR: } W_2^{1}= \\left[ {\\begin{array}{cc}-1&-1 \\end{array}} \\right], b_2^{1}=[0.5]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w^{1} = \\left[ \\begin{array}{cc} 1 & 1 \\\\ -1 & -1 \\\\ \\end{array} \\right] , b^{1}= \\left[ \\begin{array}{c} -1 \\\\ 0.5 \\\\ \\end{array} \\right]$$\n",
    "\n",
    "$$Z^{2} = w^{1}x^{1}+b^{1}= \\left[ \\begin{array}{cc} 1 & 1 \\\\ -1 & -1 \\\\ \\end{array} \\right]\\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ \\end{array} \\right] + \\left[ \\begin{array}{c} -1 \\\\ 0.5 \\\\ \\end{array} \\right] = \\left[ \\begin{array}{c} -1 \\\\ 0.5 \\\\ \\end{array} \\right], a^{2}= I(z^{2}>0) = \\left[ \\begin{array}{c} 0 \\\\ 1 \\\\ \\end{array} \\right]$$\n",
    "\n",
    "$$\\texttt{ XOR: } W^{2}= \\left[ {\\begin{array}{cc}1&1 \\end{array}} \\right], b^{2}=[-0.5]$$\n",
    "$$Z^{3} = w^{2}a^{2}+b^{2}= b^{1}= \\left[ {\\begin{array}{cc}1&1 \\end{array}} \\right] \\left[ \\begin{array}{c} 0 \\\\ 1 \\\\ \\end{array} \\right] -0.5 = [0.5], \\hat{y} = I(z^{3}>0)=[1] $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w^{1} = \\left[ \\begin{array}{cc} 1 & 1 \\\\ -1 & -1 \\\\ \\end{array} \\right] , b^{1}= \\left[ \\begin{array}{c} -1 \\\\ 0.5 \\\\ \\end{array} \\right]$$\n",
    "\n",
    "$$Z^{2} = w^{1}x^{1}+b^{1}= \\left[ \\begin{array}{cc} 1 & 1 \\\\ -1 & -1 \\\\ \\end{array} \\right]\\left[ \\begin{array}{c} 1 \\\\ 0 \\\\ \\end{array} \\right] + \\left[ \\begin{array}{c} -1 \\\\ 0.5 \\\\ \\end{array} \\right] = \\left[ \\begin{array}{c} 0 \\\\ -0.5 \\\\ \\end{array} \\right], a^{2}= I(z^{2}>0) = \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ \\end{array} \\right]$$\n",
    "\n",
    "$$\\texttt{ XOR: } W^{2}= \\left[ {\\begin{array}{cc} 1 & 1 \\end{array}} \\right], b^{2}=[-0.5]$$\n",
    "$$Z^{3} = w^{2}a^{2}+b^{2}= b^{1}= \\left[ {\\begin{array}{cc}1&1 \\end{array}} \\right] \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ \\end{array} \\right] -0.5 = [-0.5], \\hat{y} = I(z^{3}>0)=[0] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w^{1} = \\left[ \\begin{array}{cc} 1 & 1 \\\\ -1 & -1 \\\\ \\end{array} \\right] , b^{1}= \\left[ \\begin{array}{c} -1 \\\\ 0.5 \\\\ \\end{array} \\right]$$\n",
    "\n",
    "$$Z^{2} = w^{1}x^{1}+b^{1}= \\left[ \\begin{array}{cc} 1 & 1 \\\\ -1 & -1 \\\\ \\end{array} \\right]\\left[ \\begin{array}{c} 0 \\\\ 1 \\\\ \\end{array} \\right] + \\left[ \\begin{array}{c} -1 \\\\ 0.5 \\\\ \\end{array} \\right] = \\left[ \\begin{array}{c} 0 \\\\ -0.5 \\\\ \\end{array} \\right], a^{2}= I(z^{2}>0) = \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ \\end{array} \\right]$$\n",
    "\n",
    "$$\\texttt{ XOR: } W^{2}= \\left[ {\\begin{array}{cc} 1 & 1 \\end{array}} \\right], b^{2}=[-0.5]$$\n",
    "$$Z^{3} = w^{2}a^{2}+b^{2}= b^{1}= \\left[ {\\begin{array}{cc}1&1 \\end{array}} \\right] \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ \\end{array} \\right] -0.5 = [-0.5] , \\hat{y} = I(z^{3}>0)=[0] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w^{1} = \\left[ \\begin{array}{cc} 1 & 1 \\\\ -1 & -1 \\\\ \\end{array} \\right] , b^{1}= \\left[ \\begin{array}{c} -1 \\\\ 0.5 \\\\ \\end{array} \\right]$$\n",
    "\n",
    "$$Z^{2} = w^{1}x^{1}+b^{1}= \\left[ \\begin{array}{cc} 1 & 1 \\\\ -1 & -1 \\\\ \\end{array} \\right]\\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\end{array} \\right] + \\left[ \\begin{array}{c} -1 \\\\ 0.5 \\\\ \\end{array} \\right] = \\left[ \\begin{array}{c} 1 \\\\ -1.5 \\\\ \\end{array} \\right], a^{2}= I(z^{2}>0) = \\left[ \\begin{array}{c} 1 \\\\ 0 \\\\ \\end{array} \\right]$$\n",
    "\n",
    "$$\\texttt{ XOR: } W^{2}= \\left[ {\\begin{array}{cc} 1 & 1 \\end{array}} \\right], b^{2}=[-0.5]$$\n",
    "$$Z^{3} = w^{2}a^{2}+b^{2}= b^{1}= \\left[ {\\begin{array}{cc}1&1 \\end{array}} \\right] \\left[ \\begin{array}{c} 1 \\\\ 0 \\\\ \\end{array} \\right] -0.5 = [-0.5] , \\hat{y} = I(z^{3}>0)=[1] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Problem 2 - Back Propagation and Deep Networks\n",
    "***\n",
    "\n",
    "In this problem you'll gain some intuition about why training deep neural networks can be very time consuming.  Consider training the chain-like neural network seen below: \n",
    "\n",
    "![chain-like nn](figs/chain_net.png)\n",
    "\n",
    "Note that this network has three weights $W^1, W^2, W^3$ and three biases $b^1, b^2,$ and $b^3$ (for this problem you can think of each parameter as a single value or as a $1 \\times 1$ matrix). Suppose that each hidden and output neuron is equipped with a sigmoid activation function and the loss function is given by \n",
    "\n",
    "$$\n",
    "C(y, a^4) = \\frac{1}{2}(y - a^4)^2  \n",
    "$$\n",
    "\n",
    "where $a^4$ is the value of the activation at the output neuron and $y \\in \\{0,1\\}$ is the true label associated with the training example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Suppose each of the weights is initialized to $W^k = 1.0$ and each bias is initialized to $b^k = -0.5$.  Use forward propagation to find the activities and activations associated with each hidden and output neuron for the training example $(x, y) = (0.5,0)$. Show your work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\\text{Luckily, we have $1 \\times 1$ matrix, and the g() is sigmoid, not indicator}$$\n",
    "$$Z^{2} = W^{1}X + b = [1.0][0.5] - 0.5 = 0$$\n",
    "$$a^{2} = sigmoid(g(Z^{2} > 0)) = 0.5$$\n",
    "$$Z^{3} = W^{2}X + b = [1.0][0.5] - 0.5 = 0$$\n",
    "$$a^{3} = g(Z^{3}>0) = 0.5$$\n",
    "$$Z^{4} = W^{3}X + b = [1.0][0.5] - 0.5 = 0$$\n",
    "$$a^{4} = g(Z^{4}>0) = 0.5$$\n",
    "$$\\text{meaning}$$\n",
    "$$\\text{output layer}(\\hat{Y}) = 1$$\n",
    "$$$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Compute the value of $\\delta^4$ associated with the given training example. Show all work.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\\delta^4 = \\frac{\\partial C}{\\partial Z^4} = \\frac{\\partial C}{\\partial a^4} \\frac{\\partial a^4}{\\partial Z^3} $$\n",
    "$$= (a^4 - y)\\odot g(Z^4) \\odot (1-g(Z^4))$$\n",
    "$$= (0.5 - 0) \\odot 0.5 \\odot (1-0.5)$$\n",
    "$$= 0.5^3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Use Back-Propagation to compute the weight and bias derivatives $\\partial C / \\partial W^k$ and $\\partial C / \\partial b^k$ for $k=1, 2, 3$.  Show all work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\\text{The formula of finding $\\delta^L$ is $\\delta^L$ = $\\triangledown_{a^{L}}C \\odot g'(z^{L}) $ for the last layer}$$\n",
    "$$\\text{And for the next back propagation of layer is}$$\n",
    "$$\\text{For } l = L -1, \\cdots 1:$$\n",
    "$$\\frac{\\partial{C}}{\\partial{w^{l}}} = \\delta^{l+1}(a^{l})^{T} $$\n",
    "$$\\frac{\\partial{C}}{\\partial{b^{l}}} = \\delta^{l+1} $$\n",
    "$$\\delta^{l} = (W^{l})^{T} \\delta^{l+1} \\odot g'(z^{l}) $$\n",
    "\n",
    "\n",
    "$$\\text{$\\triangledown_{a^{L}}C$ is gradient so we should take derivative of C with respects to $a^{L}$ where $L=4$ and $C(y, a^4) = \\frac{1}{2}(y - a^4)^2$  }$$\n",
    "$$\\frac{\\partial{C(y, a^4)}}{\\partial{a^4}} = (y-a^4)(-1) = a^4-y \\text{ where } a^4 = 0.5 \\text{ from above and true y is } 0 \\text{ so } 0.5 - 0 = 0.5$$\n",
    "$$\\text{And } g'(Z^{L}) = g(Z^{4})(1-g(Z^{4}))$$\n",
    "\n",
    "\n",
    "$$\\text{,then when }L=4$$\n",
    "$$\\delta^{4} = \\frac{\\partial{C(y, a^4)}}{\\partial{a^4}} \\odot g(Z^{4})(1-g(Z^{4})) = 0.5 \\odot 0.5(1-0.5) = 0.125$$\n",
    "\n",
    "$$\\text{}$$\n",
    "$$l=3 , \\frac{\\partial{C}}{\\partial{W^{3}}} = \\delta^{4}(a^{3})^{T} = (0.125)(0.5) = 0.0625 $$\n",
    "$$\\frac{\\partial{C}}{b^{3}} = \\delta^{3+1} = \\delta^{4} = 0.125 $$\n",
    "$$\\delta^{3} = (W^{3})^{T}\\delta^{4} \\odot g(Z^{3})(1-g(Z^{3})) = 0.125 \\odot 0.25 = 0.03125 $$\n",
    "\n",
    "$$\\text{}$$\n",
    "$$l=2 , \\frac{\\partial{C}}{\\partial{W^{2}}} = \\delta^{3}(a^{2})^{T} = (0.03125)(0.5) = 0.015625 $$\n",
    "$$\\frac{\\partial{C}}{b^{2}} = \\delta^{2+1} = \\delta^{3} = 0.03125 $$\n",
    "$$\\delta^{2} = (W^{2})^{T}\\delta^{3} \\odot g(Z^{2})(1-g(Z^{2})) = 0.03125 \\odot 0.25 = 0.0078125 $$\n",
    "\n",
    "$$\\text{}$$\n",
    "\n",
    "$$\\text{Maybe we do not need to do }\\delta^{1}???$$\n",
    "$$l=1 , \\frac{\\partial{C}}{\\partial{W^{1}}} = \\delta^{2}(a^{1})^{T} = (0.0078125)(0.5) = 0.00390625 $$\n",
    "$$\\frac{\\partial{C}}{b^{1}} = \\delta^{1+1} = \\delta^{2} = 0.0078125 $$\n",
    "$$\\delta^{1} = (W^{1})^{T}\\delta^{2} \\odot g(Z^{1})(1-g(Z^{1})) = 0.0078125 \\odot 0.25 = 0.001836 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Comment on your observations in **Part C**.  In particular, compare the rate at which weights will be learned in the earlier layers to the later layers.  What would happen if we had an even deeper network? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "$$\\text{The early layers learn much slower than the later layers. Also, the deeper you make the network the slower it learns.}$$\n",
    "$$\\text{As it back propagate each layer's }\\delta \\text{ is less and less until in the first layer in one we calculated}$$\n",
    "$$\\text{Because a small change in the first layer compounds in all the layers after it like some kind of butterfly effect,}$$\n",
    "$$\\text{so the network thinks it only needs to change those weights by a very small amount to have a large impact on the prediction}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [40 points] Problem 3: Building and Training a Feed-Forward Neural Network \n",
    "***\n",
    "\n",
    "In this problem you'll implement a general feed-forward neural network class that utilizes sigmoid activation functions. Your tasks will be to implement `forward propagation`, `prediction`, `back propagation`, and a general `train` routine to learn the weights in your network via Stochastic Gradient Descent.  \n",
    "\n",
    "The skeleton for the `Network` class is below. Note that this class is almost identical to the one you worked with in the **Hands-On Neural Network** in-class notebook, so you should look there to remind yourself of the details.   Scroll down to find more information about your tasks as well as unit tests. \n",
    "\n",
    "**Important Note**: In **Problem 4** we'll be using the `Network` class to train a network to do handwritten digit recognition.  Please make sure to utilize vectorized Numpy routines as much as possible, as writing inefficient code here will cause very slow training times in **Problem 4**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:10:15.724906Z",
     "start_time": "2018-03-13T09:10:15.345793Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network \n",
    "        \n",
    "        :param sizes: a list of the number of neurons in each layer \n",
    "        \"\"\"\n",
    "        # save the number of layers in the network \n",
    "        self.L = len(sizes) \n",
    "        \n",
    "        # store the list of layer sizes \n",
    "        self.sizes = sizes  \n",
    "        \n",
    "        # initialize the bias vectors for each hidden and output layer \n",
    "        self.b = [np.random.randn(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the matrices of weights for each hidden and output layer \n",
    "        self.W = [np.random.randn(n, m) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the derivatives of biases for backprop \n",
    "        self.db = [np.zeros(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the derivatives of weights for backprop \n",
    "        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the activities on each hidden and output layer \n",
    "        self.z = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the activations on each hidden and output layer \n",
    "        self.a = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the deltas on each hidden and output layer \n",
    "        self.delta = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        self.train_accuracy = []\n",
    "        self.valid_accuracy = []\n",
    "        \n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply activation to \n",
    "        \"\"\"\n",
    "        z = np.clip(z, -20, 20)\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply derivative of activation to \n",
    "        \"\"\"\n",
    "        return self.g(z) * (1.0 - self.g(z))\n",
    "    \n",
    "    def gradC(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate gradient of cost function for squared-loss C(a,y) = (a-y)^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        take an feature vector and propagate through network \n",
    "        \n",
    "        :param x: input feature vector \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: Initialize activation on initial layer to x \n",
    "        self.a[0] = x\n",
    "        \n",
    "        # TODO: Loop over layers and compute activities and activations \n",
    "        for ll in range(self.L - 1):\n",
    "            self.z[ll + 1] = np.dot(self.W[ll], self.a[ll]) + self.b[ll]\n",
    "            self.a[ll + 1] = self.g(self.z[ll + 1])\n",
    "            \n",
    "#         self.a = self.a\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts on the the data in X. Assume at least two output neurons so predictions\n",
    "        are one-hot encoded vectorized labels. \n",
    "        \n",
    "        :param X: a matrix of data to make predictions on \n",
    "        :return y: a matrix of vectorized labels \n",
    "        \"\"\"\n",
    "       # print(\"X: \", X)\n",
    "        yhat = np.zeros((X.shape[0], self.sizes[-1]), dtype=int)\n",
    "#         print(\"yhat: \", yhat)\n",
    "        \n",
    "        # TODO: Populate yhat with one-hot-coded predictions \n",
    "        for i, row in enumerate(X):\n",
    "            \n",
    "#             for ii in range(self.L - 1):\n",
    "#                 z = np.dot(self.W[ll], a) + self.b[ll]\n",
    "#                 a = self.g(z)\n",
    "#             yaht[i][np.argmax(a)]\n",
    "\n",
    "            self.forward_prop(row)\n",
    "            \n",
    "            m = max(self.a[-1])\n",
    "            # anywhere in the last column of a equal the maximum element is 1, the rest of 0\n",
    "            b = np.where(self.a[-1] == m, 1, 0)\n",
    "            yhat[i] = b\n",
    "        \n",
    "        return yhat \n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        compute accuracy on labeled training set \n",
    "\n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vectorized true labels \n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        return np.sum(np.all(np.equal(yhat, y), axis=1)) / X.shape[0]\n",
    "            \n",
    "            \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation to get derivatives of C wrt weights and biases for given training example\n",
    "        \n",
    "        :param x: training features  \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: forward prop training example to fill in activities and activations \n",
    "        self.forward_prop(x)\n",
    "        \n",
    "        # TODO: compute deltas on output layer \n",
    "        self.delta[-1] = self.gradC(self.a[-1], y) * self.g_prime(self.z[-1])\n",
    "        \n",
    "        # TODO: loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "        for ll in range(self.L - 2, -1, -1): #loop backward\n",
    "            self.dW[ll] = np.outer(self.delta[ll + 1], self.a[ll])\n",
    "            self.db[ll] = self.delta[ll + 1]\n",
    "            self.delta[ll] = np.dot(self.W[ll].transpose(), self.delta[ll + 1]) * self.g_prime(self.z[ll])\n",
    "            \n",
    "            \n",
    "    def train(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "        \"\"\"\n",
    "        Train the network with SGD \n",
    "        \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        :param eta: learning rate \n",
    "        :param lam: regularization strength \n",
    "        :param num_epochs: number of epochs to run \n",
    "        :param isPrint: flag indicating to print training progress or not \n",
    "        \"\"\"\n",
    "        \n",
    "        counter = 1\n",
    "        \n",
    "        # initialize shuffled indices \n",
    "        shuffled_inds = list(range(X_train.shape[0]))\n",
    "        \n",
    "        # loop over training epochs \n",
    "        for ep in range(num_epochs):\n",
    "            \n",
    "            # shuffle indices \n",
    "            np.random.shuffle(shuffled_inds)\n",
    "            \n",
    "            # loop over training examples \n",
    "            for ind in shuffled_inds:\n",
    "                \n",
    "                # TODO: back prop to get derivatives \n",
    "                self.back_prop(X_train[ind, :], y_train[ind, :])\n",
    "        \n",
    "                # TODO: update weights and biases \n",
    "#                 self.W = [Wll - eta * dWll for Wll, dWll in zip(self.W, self.dW )]\n",
    "\n",
    "                self.W = [Wll - eta * (dWll + Wll*lam) for Wll, dWll in zip(self.W, self.dW )]\n",
    "                \n",
    "#                 (sum(self.dW[0]**2 * lam/2))\n",
    "                \n",
    "                self.b = [bll - eta * dbll for bll, dbll in zip(self.b, self.db)]\n",
    "            #print(self.W)\n",
    "            \n",
    "            # occasionally print accuracy\n",
    "            if isPrint and ((ep+1)%5)==1:\n",
    "                self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "                \n",
    "        # print final accuracy\n",
    "        if isPrint:\n",
    "            self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "                    \n",
    "    def epoch_report(self, ep, num_epochs, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\"\n",
    "        Print the accuracy for the given epoch on training and validation data \n",
    "        \n",
    "        :param ep: the current epoch \n",
    "        :param num_epochs: the total number of epochs\n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"epoch {:3d}/{:3d}: \".format(ep+1, num_epochs), end=\"\")\n",
    "        print(\"  train acc: {:8.3f}\".format(self.accuracy(X_train, y_train)), end=\"\")\n",
    "        \n",
    "        # This part is for Problem 4\n",
    "        self.train_accuracy.append(self.accuracy(X_train, y_train))\n",
    "        if X_valid is not None: print(\"  valid acc: {:8.3f}\".format(self.accuracy(X_valid, y_valid)))\n",
    "        else: print(\"\")   \n",
    "        if X_valid is not None:\n",
    "            self.valid_accuracy.append(self.accuracy(X_valid, y_valid))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Complete the `forward_prop` function in the `Network` class to implement forward propagation.  Your function should take in a single training example `x` and propagate it forward in the network, setting the activations and activities on the hidden and output layers.  When you think you're done, execute the following unit test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:10:17.610306Z",
     "start_time": "2018-03-13T09:10:17.602365Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testForwardProp (__main__.TestNN) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.004s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py \"prob 3A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Complete the `predict` function in the `Network` class to take in a matrix of features and return a matrix of one-hot-encoded label predictions. Your predictions should correspond to the output neuron with the largest activation.   \n",
    "\n",
    "Recall that our convention for encoding, e.g. the label $y=2$ in a classification problem with possible labels $y \\in \\{0,1,2,3\\}$ is \n",
    "\n",
    "$$\n",
    "y=2 \\quad \\Leftrightarrow \\quad y=\\left[0, 0, 1, 0\\right]\n",
    "$$\n",
    "\n",
    "So the equivalent matrix associated with the labels $y_1=3, y_2=2, y_3=0$ is \n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix}{3\\\\2\\\\0}\\end{bmatrix} \\quad \\Leftrightarrow \\quad \n",
    "y = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "When you think your `predict` function is working well, execute the following unit test. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:10:19.290278Z",
     "start_time": "2018-03-13T09:10:19.282762Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testPredict (__main__.TestNN) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py \"prob 3B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: OK, now it's time to implement back propagation.  Complete the function ``back_prop`` in the ``Network`` class to use a single training example to compute the derivatives of the loss function with respect to the weights and the biases. As in the **Hands-On** notebook, you may assume that the loss function for a single training example is given by \n",
    "\n",
    "$$\n",
    "C(y, {\\bf a}^L) = \\frac{1}{2}\\|y - {\\bf a}^L\\|^2  \n",
    "$$\n",
    "\n",
    "When you think you're done, execute the following unit test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:10:21.407947Z",
     "start_time": "2018-03-13T09:10:21.399745Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testBackProp (__main__.TestNN) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py \"prob 3C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: OK, now let's actually train a neural net!  Complete the missing code in ``train`` to loop over the training data in random order, call `back_prop` to get the derivatives, and then update the weights and the biases via SGD.  When you think you're done, execute the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:10:23.048222Z",
     "start_time": "2018-03-13T09:10:23.039936Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testSGD (__main__.TestNN) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py \"prob 3D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: Last but not least, we should implement $\\ell$-$2$ regularization.  Modify your `train` function to incorporate regularization of the weights (but **not** the biases) in your SGD update.  As in the Lecture 18 slides, you should assume that the cost function with regularization takes the form \n",
    "\n",
    "$$\n",
    "C_\\lambda = C + \\frac{\\lambda}{2} \\displaystyle\\sum_{w} w^2\n",
    "$$\n",
    "\n",
    "where $\\sum_{w}$ sums over each weight in all layers of the network. Think carefully before you go making large changes to your code.  This modification is much simpler than you think. When you think you're done, execute the following unit test.  (Then go back and execute the test in **Part C** to make sure you didn't break anything.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:10:25.040053Z",
     "start_time": "2018-03-13T09:10:25.031009Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testRegularizedSGD (__main__.TestNN) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests/tests.py \"prob 3E\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 4: A Neural Network Classifier for Handwritten Digit Recognition \n",
    "***\n",
    "\n",
    "In this problem you'll use the Feed-Forward Neural Network framework you wrote in **Problem 3** to take an image of a handwritten digit and predict which digit it corresponds to.  \n",
    "\n",
    "![Samples of Handwritten Digits](figs/mnist.png \"MNIST Digits\")\n",
    "\n",
    "To keep run times down we'll again only consider the subset of the MNIST data set consisting of the digits $3, 7, 8$ and $9$. \n",
    "\n",
    "**Part A**: Executing the following cells will load training and validation data and plot an example handwritten digit.  Explore the training and validation sets and answer the following questions: \n",
    "\n",
    "- How many pixels are in each image in the data set?  \n",
    "- How do the true labels correspond to the associated one-hot-encoded label vectors? \n",
    "- Give an example of a network architecture with a single hidden layer that is compatible with this data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:10:27.264422Z",
     "start_time": "2018-03-13T09:10:27.153070Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid = pickle.load(gzip.open(\"data/mnist21x21_3789_one_hot.pklz\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T09:10:28.454747Z",
     "start_time": "2018-03-13T09:10:28.350656Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNAqO0Cg4QqPgCI2CIzQKjtD+DyT5yECHP81jAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11411ae80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def view_digit(x, label=None):\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    plt.imshow(x.reshape(21,21), cmap='gray');\n",
    "    plt.xticks([]); plt.yticks([]);\n",
    "    if label: plt.xlabel(\"true: {}\".format(label), fontsize=16)\n",
    "        \n",
    "training_index = 2\n",
    "label_dict = dict({0:3, 1:7, 2:8, 3:9})\n",
    "view_digit(X_train[training_index], label_dict[np.argmax(y_train[training_index])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "441\n",
      "There are 4000 pictures, each picture has 441 pixels\n"
     ]
    }
   ],
   "source": [
    "#label_dict = dict({0:3, 1:7, 2:8, 3:9})\n",
    "\n",
    "print(len(y_train))\n",
    "print(len(X_train[0])) # 441 pixels\n",
    "\n",
    "print(\"There are 4000 pictures, each picture has 441 pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{There are 4000 pixel are in each image}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{The true label is computed using the argmax of the one-hot-encoded (index of highest  value)}$$\n",
    "$$\\text{ is plugging in dictionary is}$$\n",
    "$$[0:3, 1:7, 2:8, 3:9]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{A reasonable exmaple of network architecture with a single hidden layer would be [441, 100, 4]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Train a network with a single hidden layer containing $30$ neurons on the first $500$ training examples in the training set using a learning rate of $\\eta = 0.01$ for at least $50$ epochs.  What accuracy does your network achieve on the validation set?  Do you see any clear signs of overfitting?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1/ 50:   train acc:    0.292  valid acc:    0.234\n",
      "epoch   6/ 50:   train acc:    0.446  valid acc:    0.392\n",
      "epoch  11/ 50:   train acc:    0.466  valid acc:    0.420\n",
      "epoch  16/ 50:   train acc:    0.490  valid acc:    0.440\n",
      "epoch  21/ 50:   train acc:    0.520  valid acc:    0.476\n",
      "epoch  26/ 50:   train acc:    0.632  valid acc:    0.608\n",
      "epoch  31/ 50:   train acc:    0.736  valid acc:    0.712\n",
      "epoch  36/ 50:   train acc:    0.804  valid acc:    0.766\n",
      "epoch  41/ 50:   train acc:    0.828  valid acc:    0.800\n",
      "epoch  46/ 50:   train acc:    0.838  valid acc:    0.806\n",
      "epoch  50/ 50:   train acc:    0.858  valid acc:    0.824\n",
      "Accuracy is  0.82725\n"
     ]
    }
   ],
   "source": [
    "# 441 inputs: 441 pixel, a single hidden layer of 30 neurons, 4 outputs\n",
    "TrainNetwork = Network([441, 30, 4]) \n",
    "\n",
    "eta = 0.01\n",
    "lam = 0.0\n",
    "num_epochs = 50    \n",
    "\n",
    "#(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "\n",
    "TrainNetwork.train(X_train[:500], y_train[:500], X_valid[:500], y_valid[:500], eta = eta , num_epochs = num_epochs, isPrint=True)\n",
    "\n",
    "# TrainNetwork.train(X_train, y_train, X_valid, y_valid, eta = eta, num_epochs, isPrint=False)\n",
    "# TrainNetwork.train(X_train[:500], y_train[:500], X_valid, y_valid, eta, num_epochs, isPrint=False)\n",
    "result = TrainNetwork.accuracy(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy is \", result)\n",
    "# plt.plot(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Modify the `Network` class so that it stores the accuracies on the training and validation data every $5$ epochs during the training process. Now increase the number of neurons in the hidden layer to $100$.  On a single set of axes, plot the **validation accuracy** vs epoch for networks trained on the full training set for at least 50 epochs using the learning rates $\\eta = 0.01$, $\\eta = 0.25$ and $\\eta = 1.5$.  Which learning rate seems to perform the best? What is the best accuracy achieved on the validation set?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1/ 50:   train acc:    0.363  valid acc:    0.385\n",
      "epoch   6/ 50:   train acc:    0.686  valid acc:    0.683\n",
      "epoch  11/ 50:   train acc:    0.716  valid acc:    0.698\n",
      "epoch  16/ 50:   train acc:    0.920  valid acc:    0.895\n",
      "epoch  21/ 50:   train acc:    0.936  valid acc:    0.918\n",
      "epoch  26/ 50:   train acc:    0.943  valid acc:    0.921\n",
      "epoch  31/ 50:   train acc:    0.950  valid acc:    0.928\n",
      "epoch  36/ 50:   train acc:    0.953  valid acc:    0.931\n",
      "epoch  41/ 50:   train acc:    0.956  valid acc:    0.933\n",
      "epoch  46/ 50:   train acc:    0.957  valid acc:    0.934\n",
      "epoch  50/ 50:   train acc:    0.960  valid acc:    0.935\n",
      "eta = 0.01:  [0.385, 0.683, 0.698, 0.895, 0.918, 0.921, 0.928, 0.931, 0.933, 0.934, 0.935]\n",
      "epoch   1/ 50:   train acc:    0.507  valid acc:    0.499\n",
      "epoch   6/ 50:   train acc:    0.963  valid acc:    0.937\n",
      "epoch  11/ 50:   train acc:    0.981  valid acc:    0.949\n",
      "epoch  16/ 50:   train acc:    0.986  valid acc:    0.952\n",
      "epoch  21/ 50:   train acc:    0.989  valid acc:    0.956\n",
      "epoch  26/ 50:   train acc:    0.990  valid acc:    0.951\n",
      "epoch  31/ 50:   train acc:    0.991  valid acc:    0.952\n",
      "epoch  36/ 50:   train acc:    0.992  valid acc:    0.952\n",
      "epoch  41/ 50:   train acc:    0.993  valid acc:    0.954\n",
      "epoch  46/ 50:   train acc:    0.993  valid acc:    0.954\n",
      "epoch  50/ 50:   train acc:    0.994  valid acc:    0.955\n",
      "eta = 0.25:  [0.499, 0.937, 0.949, 0.952, 0.956, 0.951, 0.952, 0.952, 0.954, 0.954, 0.955]\n",
      "epoch   1/ 50:   train acc:    0.726  valid acc:    0.717\n",
      "epoch   6/ 50:   train acc:    0.928  valid acc:    0.916\n",
      "epoch  11/ 50:   train acc:    0.966  valid acc:    0.944\n",
      "epoch  16/ 50:   train acc:    0.970  valid acc:    0.944\n",
      "epoch  21/ 50:   train acc:    0.978  valid acc:    0.954\n",
      "epoch  26/ 50:   train acc:    0.972  valid acc:    0.953\n",
      "epoch  31/ 50:   train acc:    0.989  valid acc:    0.964\n",
      "epoch  36/ 50:   train acc:    0.982  valid acc:    0.959\n",
      "epoch  41/ 50:   train acc:    0.989  valid acc:    0.965\n",
      "epoch  46/ 50:   train acc:    0.993  valid acc:    0.969\n",
      "epoch  50/ 50:   train acc:    0.992  valid acc:    0.967\n",
      "eta = 1.5:  [0.717, 0.916, 0.944, 0.944, 0.954, 0.953, 0.964, 0.959, 0.965, 0.969, 0.967]\n"
     ]
    }
   ],
   "source": [
    "# 441 inputs: 441 pixel, a single hidden layer of 100 neurons, 4 outputs\n",
    "num_epochs = 50\n",
    "\n",
    "#(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "\n",
    "TrainNetwork1 = Network([441, 100, 4])\n",
    "eta1 = 0.01\n",
    "TrainNetwork1.train(X_train, y_train, X_valid, y_valid, eta = eta1, num_epochs = num_epochs, isPrint=True)\n",
    "print(\"eta = 0.01: \", TrainNetwork1.valid_accuracy)\n",
    "\n",
    "\n",
    "TrainNetwork2 = Network([441, 100, 4])\n",
    "eta2 = 0.25\n",
    "TrainNetwork2.train(X_train, y_train, X_valid, y_valid, eta = eta2, num_epochs = num_epochs, isPrint=True)\n",
    "print(\"eta = 0.25: \", TrainNetwork2.valid_accuracy)\n",
    "\n",
    "\n",
    "TrainNetwork3 = Network([441, 100, 4])\n",
    "eta3 = 1.5\n",
    "TrainNetwork3.train(X_train, y_train, X_valid, y_valid, eta = eta3, num_epochs = num_epochs, isPrint=True)\n",
    "print(\"eta = 1.5: \" ,TrainNetwork3.valid_accuracy)\n",
    "\n",
    "yplot1 = TrainNetwork1.valid_accuracy\n",
    "yplot2 = TrainNetwork2.valid_accuracy\n",
    "yplot3 = TrainNetwork3.valid_accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAF8CAYAAAAn/HmMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c100f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mcolors = [\"black\", \"gray\", \"blue\"]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,6))\n",
    "\n",
    "ax.plot(yplot1, color=mcolors[0], lw=3, label=\"lam = 0.01\", zorder=1)\n",
    "ax.plot(yplot2, color=mcolors[1], lw=3, label=\"lam = 0.25\", zorder=1)\n",
    "ax.plot(yplot3, color=mcolors[2], lw=3, label=\"lam = 1.5\", zorder=1)\n",
    "\n",
    "ax.set_xlabel(\"number of epochs\", fontsize=16)\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=16)\n",
    "ax.legend(loc=\"upper right\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**:  Now let's see if we can get better results with regularization. Using the best learning rate you found in **Part C**, on a single set of axes, plot the **validation accuracy** vs epoch for networks trained on the full training set for at least 50 epochs using the regularization strengths $\\lambda = 10^{-6}$, $\\lambda = 10^{-4}$ and $\\lambda = 10^{-2}$.  Which regularization strength seems to perform the best? What is the best accuracy achieved on the validation set?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1/ 50:   train acc:    0.879  valid acc:    0.882\n",
      "epoch   6/ 50:   train acc:    0.790  valid acc:    0.763\n",
      "epoch  11/ 50:   train acc:    0.891  valid acc:    0.884\n",
      "epoch  16/ 50:   train acc:    0.817  valid acc:    0.817\n",
      "epoch  21/ 50:   train acc:    0.800  valid acc:    0.788\n",
      "epoch  26/ 50:   train acc:    0.894  valid acc:    0.892\n",
      "epoch  31/ 50:   train acc:    0.892  valid acc:    0.880\n",
      "epoch  36/ 50:   train acc:    0.754  valid acc:    0.734\n",
      "epoch  41/ 50:   train acc:    0.859  valid acc:    0.834\n",
      "epoch  46/ 50:   train acc:    0.632  valid acc:    0.633\n",
      "epoch  50/ 50:   train acc:    0.798  valid acc:    0.776\n",
      "lam = 0.01:  [0.882, 0.763, 0.884, 0.817, 0.788, 0.892, 0.88, 0.734, 0.834, 0.633, 0.776]\n",
      "epoch   1/ 50:   train acc:    0.919  valid acc:    0.900\n",
      "epoch   6/ 50:   train acc:    0.972  valid acc:    0.944\n",
      "epoch  11/ 50:   train acc:    0.981  valid acc:    0.962\n",
      "epoch  16/ 50:   train acc:    0.981  valid acc:    0.950\n",
      "epoch  21/ 50:   train acc:    0.987  valid acc:    0.966\n",
      "epoch  26/ 50:   train acc:    0.982  valid acc:    0.958\n",
      "epoch  31/ 50:   train acc:    0.991  valid acc:    0.968\n",
      "epoch  36/ 50:   train acc:    0.990  valid acc:    0.965\n",
      "epoch  41/ 50:   train acc:    0.984  valid acc:    0.962\n",
      "epoch  46/ 50:   train acc:    0.989  valid acc:    0.959\n",
      "epoch  50/ 50:   train acc:    0.988  valid acc:    0.957\n",
      "lam = 0.0001:  [0.9, 0.944, 0.962, 0.95, 0.966, 0.958, 0.968, 0.965, 0.962, 0.959, 0.957]\n",
      "epoch   1/ 50:   train acc:    0.930  valid acc:    0.916\n",
      "epoch   6/ 50:   train acc:    0.980  valid acc:    0.942\n",
      "epoch  11/ 50:   train acc:    0.986  valid acc:    0.946\n",
      "epoch  16/ 50:   train acc:    0.989  valid acc:    0.949\n",
      "epoch  21/ 50:   train acc:    0.990  valid acc:    0.957\n",
      "epoch  26/ 50:   train acc:    0.991  valid acc:    0.959\n",
      "epoch  31/ 50:   train acc:    0.992  valid acc:    0.962\n",
      "epoch  36/ 50:   train acc:    0.992  valid acc:    0.963\n",
      "epoch  41/ 50:   train acc:    0.993  valid acc:    0.961\n",
      "epoch  46/ 50:   train acc:    0.994  valid acc:    0.961\n",
      "epoch  50/ 50:   train acc:    0.994  valid acc:    0.962\n",
      "lam = 0.000001:  [0.916, 0.942, 0.946, 0.949, 0.957, 0.959, 0.962, 0.963, 0.961, 0.961, 0.962]\n"
     ]
    }
   ],
   "source": [
    "#(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "num_epochs = 50\n",
    "eta = 0.25\n",
    "\n",
    "TrainNetwork1 = Network([441, 100, 4])\n",
    "lam1 = 0.01\n",
    "TrainNetwork1.train(X_train, y_train, X_valid, y_valid, eta, lam = lam1, num_epochs = num_epochs, isPrint=True)\n",
    "print(\"lam = 0.01: \", TrainNetwork1.valid_accuracy)\n",
    "\n",
    "TrainNetwork2 = Network([441, 100, 4])\n",
    "lam2 = 0.0001\n",
    "TrainNetwork2.train(X_train, y_train, X_valid, y_valid, eta, lam = lam2, num_epochs = num_epochs, isPrint=True)\n",
    "print(\"lam = 0.0001: \", TrainNetwork2.valid_accuracy)\n",
    "\n",
    "TrainNetwork3 = Network([441, 100, 4])\n",
    "lam3 = 0.000001\n",
    "TrainNetwork3.train(X_train, y_train, X_valid, y_valid, eta, lam = lam3, num_epochs = num_epochs, isPrint=True)\n",
    "print(\"lam = 0.000001: \", TrainNetwork3.valid_accuracy)\n",
    "\n",
    "\n",
    "yplot1 = TrainNetwork1.valid_accuracy\n",
    "yplot2 = TrainNetwork2.valid_accuracy\n",
    "yplot3 = TrainNetwork3.valid_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAF8CAYAAAC5cAPdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10aae0588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mcolors = [\"black\", \"gray\", \"blue\"]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,6))\n",
    "\n",
    "ax.plot(yplot1, color=mcolors[0], lw=3, label=\"lam = 0.01\", zorder=1)\n",
    "ax.plot(yplot2, color=mcolors[1], lw=3, label=\"lam = 0.0001\", zorder=1)\n",
    "ax.plot(yplot3, color=mcolors[2], lw=3, label=\"lam = 0.000001\", zorder=1)\n",
    "\n",
    "ax.set_xlabel(\"number of epochs\", fontsize=16)\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=16)\n",
    "ax.legend(loc=\"upper right\", fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**:  Now let's see if we can get better results with different network architectures. On a single set of axes, plot the **validation accuracy** vs epoch for networks trained on the full training set for at least 50 epochs using the architecture from **Part D** as well as two other architectures.  Which architecture seems to perform the best? What is the best accuracy achieved on the validation set?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with one hidden layer:  0.97775\n",
      "Accuracy with two hidden layer:  0.98875\n",
      "Accuracy with three hidden layer:  0.92975\n"
     ]
    }
   ],
   "source": [
    "#Network([441, 100, 4]) = 441 inputs, one hidden layer with 100 neurons, and 4 output layers\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "TrainNetwork1 = Network([441, 100, 4])\n",
    "eta1 = 1\n",
    "lam1 = 0.0001\n",
    "TrainNetwork1.train(X_train, y_train, X_valid, y_valid, eta = eta1, lam = lam1, num_epochs = num_epochs, isPrint=False)\n",
    "result1 = TrainNetwork1.accuracy(X_train, y_train)\n",
    "\n",
    "# print(\"With one hidden layer: \", TrainNetwork1.Eta1Lam1)\n",
    "print(\"Accuracy with one hidden layer: \", result1)\n",
    "\n",
    "TrainNetwork2 = Network([441, 50, 60, 4])\n",
    "\n",
    "TrainNetwork2.train(X_train, y_train, X_valid, y_valid, eta = eta2, lam = lam2, num_epochs = num_epochs, isPrint=False)\n",
    "result2 = TrainNetwork2.accuracy(X_train, y_train)\n",
    "\n",
    "# print(\"With two hidden layers: \", TrainNetwork2.Eta2Lam2)\n",
    "print(\"Accuracy with two hidden layer: \", result2)\n",
    "\n",
    "\n",
    "TrainNetwork3 = Network([441, 70, 70, 75, 4])\n",
    "\n",
    "TrainNetwork3.train(X_train, y_train, X_valid, y_valid, eta = eta3, lam = lam3, num_epochs = num_epochs, isPrint=False)\n",
    "result3 = TrainNetwork3.accuracy(X_train, y_train)\n",
    "\n",
    "# print(\"With three hidden layers: \", TrainNetwork3.Eta3Lam3)\n",
    "print(\"Accuracy with three hidden layer: \", result3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\\text{The best architectures is the two hidden layers, and the accuracy is 98%}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [max 10 points] Extra Credit: Improving Network Performance \n",
    "***\n",
    "\n",
    "See if you can get better performance by exploring advanced techniques.  Things you might try are: \n",
    "\n",
    "- Implementing **Mini-Batch** Stochastic Gradient Descent \n",
    "- Experimenting with different activation functions (like tanh and ReLU)\n",
    "- Experimenting with different loss functions (like cross-entropy or softmax) \n",
    "\n",
    "For more detailed discussion of these techniques it'll be helpful to look at Chapter 3 of [Nielsen](http://neuralnetworksanddeeplearning.com/chap3.html). \n",
    "\n",
    "To receive the extra credit you should try at least a couple of the above and clearly describe what worked and what did not.  \n",
    "\n",
    "**Important Note**: Don't do any of these things in the original `Network` class, because you'll almost certainly break the unit tests.  Copy the `Network` class from above and rename it `BetterNetwork` (or something) and modify the new class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BettetNetwork:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network \n",
    "        \n",
    "        :param sizes: a list of the number of neurons in each layer \n",
    "        \"\"\"\n",
    "        # save the number of layers in the network \n",
    "        self.L = len(sizes) \n",
    "        \n",
    "        # store the list of layer sizes \n",
    "        self.sizes = sizes  \n",
    "        \n",
    "        # initialize the bias vectors for each hidden and output layer \n",
    "        self.b = [np.random.randn(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the matrices of weights for each hidden and output layer \n",
    "        self.W = [np.random.randn(n, m) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the derivatives of biases for backprop \n",
    "        self.db = [np.zeros(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the derivatives of weights for backprop \n",
    "        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the activities on each hidden and output layer \n",
    "        self.z = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the activations on each hidden and output layer \n",
    "        self.a = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the deltas on each hidden and output layer \n",
    "        self.delta = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        \n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply activation to \n",
    "        \"\"\"\n",
    "        z = np.clip(z, -20, 20)\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply derivative of activation to \n",
    "        \"\"\"\n",
    "        return self.g(z) * (1.0 - self.g(z))\n",
    "    \n",
    "    def gradC(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate gradient of cost function for squared-loss C(a,y) = (a-y)^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        take an feature vector and propagate through network \n",
    "        \n",
    "        :param x: input feature vector \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: Initialize activation on initial layer to x \n",
    "        self.a[0] = x\n",
    "        \n",
    "        # TODO: Loop over layers and compute activities and activations \n",
    "        for ll in range(self.L - 1):\n",
    "            self.z[ll + 1] = np.dot(self.W[ll], self.a[ll]) + self.b[ll]\n",
    "            self.a[ll + 1] = self.g(self.z[ll + 1])\n",
    "            \n",
    "#         self.a = self.a\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts on the the data in X. Assume at least two output neurons so predictions\n",
    "        are one-hot encoded vectorized labels. \n",
    "        \n",
    "        :param X: a matrix of data to make predictions on \n",
    "        :return y: a matrix of vectorized labels \n",
    "        \"\"\"\n",
    "       # print(\"X: \", X)\n",
    "        yhat = np.zeros((X.shape[0], self.sizes[-1]), dtype=int)\n",
    "#         print(\"yhat: \", yhat)\n",
    "        \n",
    "        # TODO: Populate yhat with one-hot-coded predictions \n",
    "        for i, row in enumerate(X):\n",
    "            self.forward_prop(row)\n",
    "            \n",
    "            m = max(self.a[-1])\n",
    "            # anywhere in the last column of a equal the maximum element is 1, the rest of 0\n",
    "            b = np.where(self.a[-1] == m, 1, 0)\n",
    "            yhat[i] = b\n",
    "        \n",
    "        return yhat \n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        compute accuracy on labeled training set \n",
    "\n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vectorized true labels \n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        return np.sum(np.all(np.equal(yhat, y), axis=1)) / X.shape[0]\n",
    "            \n",
    "            \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation to get derivatives of C wrt weights and biases for given training example\n",
    "        \n",
    "        :param x: training features  \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: forward prop training example to fill in activities and activations \n",
    "        self.forward_prop(x)\n",
    "        \n",
    "        # TODO: compute deltas on output layer \n",
    "        self.delta[-1] = self.gradC(self.a[-1], y) * self.g_prime(self.z[-1])\n",
    "        \n",
    "        # TODO: loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "        for ll in range(self.L - 2, -1, -1): #loop backward\n",
    "            self.dW[ll] = np.outer(self.delta[ll + 1], self.a[ll])\n",
    "            self.db[ll] = self.delta[ll + 1]\n",
    "            self.delta[ll] = np.dot(self.W[ll].transpose(), self.delta[ll + 1]) * self.g_prime(self.z[ll])\n",
    "            \n",
    "            \n",
    "    def train(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "        \"\"\"\n",
    "        Train the network with SGD \n",
    "        \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        :param eta: learning rate \n",
    "        :param lam: regularization strength \n",
    "        :param num_epochs: number of epochs to run \n",
    "        :param isPrint: flag indicating to print training progress or not \n",
    "        \"\"\"\n",
    "        \n",
    "        counter = 1\n",
    "        \n",
    "        # initialize shuffled indices \n",
    "        shuffled_inds = list(range(X_train.shape[0]))\n",
    "        \n",
    "        # loop over training epochs \n",
    "        for ep in range(num_epochs):\n",
    "            \n",
    "            # shuffle indices \n",
    "            np.random.shuffle(shuffled_inds)\n",
    "            \n",
    "            # loop over training examples \n",
    "            for ind in shuffled_inds:\n",
    "                \n",
    "                # TODO: back prop to get derivatives \n",
    "                self.back_prop(X_train[ind, :], y_train[ind, :])\n",
    "        \n",
    "                # TODO: update weights and biases \n",
    "#                 self.W = [Wll - eta * dWll for Wll, dWll in zip(self.W, self.dW )]\n",
    "\n",
    "                self.W = [Wll - (eta) * (dWll + Wll*lam) for Wll, dWll in zip(self.W, self.dW )]\n",
    "                \n",
    "#                 (sum(self.dW[0]**2 * lam/2))\n",
    "                \n",
    "                self.b = [bll - (eta) * dbll for bll, dbll in zip(self.b, self.db)]\n",
    "            #print(self.W)   \n",
    "    \n",
    "            # occasionally print accuracy\n",
    "            if isPrint and ((ep+1)%10)==1:\n",
    "                self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "                \n",
    "        # print final accuracy\n",
    "        if isPrint:\n",
    "            self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "                    \n",
    "    def epoch_report(self, ep, num_epochs, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\"\n",
    "        Print the accuracy for the given epoch on training and validation data \n",
    "        \n",
    "        :param ep: the current epoch \n",
    "        :param num_epochs: the total number of epochs\n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"epoch {:3d}/{:3d}: \".format(ep+1, num_epochs), end=\"\")\n",
    "        print(\"  train acc: {:8.3f}\".format(self.accuracy(X_train, y_train)), end=\"\")\n",
    "        if X_valid is not None: print(\"  valid acc: {:8.3f}\".format(self.accuracy(X_valid, y_valid)))\n",
    "        else: print(\"\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TanhNetwork:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network \n",
    "        \n",
    "        :param sizes: a list of the number of neurons in each layer \n",
    "        \"\"\"\n",
    "        # save the number of layers in the network \n",
    "        self.L = len(sizes) \n",
    "        \n",
    "        # store the list of layer sizes \n",
    "        self.sizes = sizes  \n",
    "        \n",
    "        # initialize the bias vectors for each hidden and output layer \n",
    "        self.b = [np.random.randn(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the matrices of weights for each hidden and output layer \n",
    "        self.W = [np.random.randn(n, m) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the derivatives of biases for backprop \n",
    "        self.db = [np.zeros(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the derivatives of weights for backprop \n",
    "        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the activities on each hidden and output layer \n",
    "        self.z = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the activations on each hidden and output layer \n",
    "        self.a = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the deltas on each hidden and output layer \n",
    "        self.delta = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "#     def relu_activation(X):\n",
    "#         return np.maximum(X, 0)\n",
    "    \n",
    "#     def softmax(output_arr):\n",
    "#         logis_exp = np.exe(output_arr)\n",
    "#         return logis_exp / np.sum(logis_exp, axis = 1, )\n",
    "        \n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply activation to \n",
    "        \"\"\"\n",
    "        \n",
    "        z = np.clip(z, -20, 20)\n",
    "        return np.sinh(z)/np.cosh(z)\n",
    "    \n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply derivative of activation to \n",
    "        \"\"\"\n",
    "        return 1 - self.g(z)**2\n",
    "    \n",
    "    def gradC(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate gradient of cost function for squared-loss C(a,y) = (a-y)^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        take an feature vector and propagate through network \n",
    "        \n",
    "        :param x: input feature vector \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: Initialize activation on initial layer to x \n",
    "        self.a[0] = x\n",
    "        \n",
    "        # TODO: Loop over layers and compute activities and activations \n",
    "        for ll in range(self.L - 1):\n",
    "            self.z[ll + 1] = np.dot(self.W[ll], self.a[ll]) + self.b[ll]\n",
    "            self.a[ll + 1] = self.g(self.z[ll + 1])\n",
    "            \n",
    "#         self.a = self.a\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts on the the data in X. Assume at least two output neurons so predictions\n",
    "        are one-hot encoded vectorized labels. \n",
    "        \n",
    "        :param X: a matrix of data to make predictions on \n",
    "        :return y: a matrix of vectorized labels \n",
    "        \"\"\"\n",
    "       # print(\"X: \", X)\n",
    "        yhat = np.zeros((X.shape[0], self.sizes[-1]), dtype=int)\n",
    "#         print(\"yhat: \", yhat)\n",
    "        \n",
    "        # TODO: Populate yhat with one-hot-coded predictions \n",
    "        for i, row in enumerate(X):\n",
    "            self.forward_prop(row)\n",
    "            \n",
    "            m = max(self.a[-1])\n",
    "            # anywhere in the last column of a equal the maximum element is 1, the rest of 0\n",
    "            b = np.where(self.a[-1] == m, 1, 0)\n",
    "            yhat[i] = b\n",
    "        \n",
    "        return yhat \n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        compute accuracy on labeled training set \n",
    "\n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vectorized true labels \n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        return np.sum(np.all(np.equal(yhat, y), axis=1)) / X.shape[0]\n",
    "            \n",
    "            \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation to get derivatives of C wrt weights and biases for given training example\n",
    "        \n",
    "        :param x: training features  \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: forward prop training example to fill in activities and activations \n",
    "        self.forward_prop(x)\n",
    "        \n",
    "        # TODO: compute deltas on output layer \n",
    "        self.delta[-1] = self.gradC(self.a[-1], y) * self.g_prime(self.z[-1])\n",
    "        \n",
    "        # TODO: loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "        for ll in range(self.L - 2, -1, -1): #loop backward\n",
    "            self.dW[ll] = np.outer(self.delta[ll + 1], self.a[ll])\n",
    "            self.db[ll] = self.delta[ll + 1]\n",
    "            self.delta[ll] = np.dot(self.W[ll].transpose(), self.delta[ll + 1]) * self.g_prime(self.z[ll])\n",
    "            \n",
    "            \n",
    "    def train(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "        \"\"\"\n",
    "        Train the network with SGD \n",
    "        \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        :param eta: learning rate \n",
    "        :param lam: regularization strength \n",
    "        :param num_epochs: number of epochs to run \n",
    "        :param isPrint: flag indicating to print training progress or not \n",
    "        \"\"\"\n",
    "        \n",
    "        counter = 1\n",
    "        \n",
    "        # initialize shuffled indices \n",
    "        shuffled_inds = list(range(X_train.shape[0]))\n",
    "        \n",
    "        # loop over training epochs \n",
    "        for ep in range(num_epochs):\n",
    "            \n",
    "            # shuffle indices \n",
    "            np.random.shuffle(shuffled_inds)\n",
    "            \n",
    "            # loop over training examples \n",
    "            for ind in shuffled_inds:\n",
    "                \n",
    "                # TODO: back prop to get derivatives \n",
    "                self.back_prop(X_train[ind, :], y_train[ind, :])\n",
    "        \n",
    "                # TODO: update weights and biases \n",
    "#                 self.W = [Wll - eta * dWll for Wll, dWll in zip(self.W, self.dW )]\n",
    "\n",
    "                self.W = [Wll - (eta) * (dWll + Wll*lam) for Wll, dWll in zip(self.W, self.dW )]\n",
    "                \n",
    "#                 (sum(self.dW[0]**2 * lam/2))\n",
    "                \n",
    "                self.b = [bll - (eta) * dbll for bll, dbll in zip(self.b, self.db)]\n",
    "            #print(self.W)   \n",
    "    \n",
    "            # occasionally print accuracy\n",
    "            if isPrint and ((ep+1)%10)==1:\n",
    "                self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "                \n",
    "        # print final accuracy\n",
    "        if isPrint:\n",
    "            self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "                    \n",
    "    def epoch_report(self, ep, num_epochs, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\"\n",
    "        Print the accuracy for the given epoch on training and validation data \n",
    "        \n",
    "        :param ep: the current epoch \n",
    "        :param num_epochs: the total number of epochs\n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"epoch {:3d}/{:3d}: \".format(ep+1, num_epochs), end=\"\")\n",
    "        print(\"  train acc: {:8.3f}\".format(self.accuracy(X_train, y_train)), end=\"\")\n",
    "        if X_valid is not None: print(\"  valid acc: {:8.3f}\".format(self.accuracy(X_valid, y_valid)))\n",
    "        else: print(\"\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1/ 50:   train acc:    0.457  valid acc:    0.429\n",
      "epoch  11/ 50:   train acc:    0.880  valid acc:    0.879\n",
      "epoch  21/ 50:   train acc:    0.893  valid acc:    0.879\n",
      "epoch  31/ 50:   train acc:    0.915  valid acc:    0.902\n",
      "epoch  41/ 50:   train acc:    0.925  valid acc:    0.907\n",
      "epoch  50/ 50:   train acc:    0.933  valid acc:    0.911\n",
      "Accuracy is  0.93275\n"
     ]
    }
   ],
   "source": [
    "# 441 inputs: 441 pixel, a single hidden layer of 30 neurons, 4 outputs\n",
    "TanhTrainNetwork = TanhNetwork([441, 30, 4]) \n",
    "\n",
    "eta = 0.01\n",
    "lam = 0.0\n",
    "num_epochs = 50    \n",
    "\n",
    "#(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "\n",
    "TanhTrainNetwork.train(X_train, y_train, X_valid, y_valid, eta = eta , num_epochs = num_epochs, isPrint=True)\n",
    "\n",
    "# TrainNetwork.train(X_train, y_train, X_valid, y_valid, eta = eta, num_epochs, isPrint=False)\n",
    "# TrainNetwork.train(X_train[:500], y_train[:500], X_valid, y_valid, eta, num_epochs, isPrint=False)\n",
    "result = TanhTrainNetwork.accuracy(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy is \", result)\n",
    "# plt.plot(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Tanh function is working great! Tanh function is also sigmoidal. Strongly negative inputs to the tanh will map to negative outputs}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluNetwork:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network \n",
    "        \n",
    "        :param sizes: a list of the number of neurons in each layer \n",
    "        \"\"\"\n",
    "        # save the number of layers in the network \n",
    "        self.L = len(sizes) \n",
    "        \n",
    "        # store the list of layer sizes \n",
    "        self.sizes = sizes  \n",
    "        \n",
    "        # initialize the bias vectors for each hidden and output layer \n",
    "        self.b = [np.random.randn(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the matrices of weights for each hidden and output layer \n",
    "        self.W = [np.random.randn(n, m) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the derivatives of biases for backprop \n",
    "        self.db = [np.zeros(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the derivatives of weights for backprop \n",
    "        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the activities on each hidden and output layer \n",
    "        self.z = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the activations on each hidden and output layer \n",
    "        self.a = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the deltas on each hidden and output layer \n",
    "        self.delta = [np.zeros(n) for n in self.sizes]\n",
    "\n",
    "        \n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply activation to \n",
    "        \"\"\"\n",
    "        \n",
    "        z = np.clip(z, -20, 20)\n",
    "        \n",
    "        return z * (z > 0)\n",
    "#         return np.maximum(z, 0)\n",
    "    \n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply derivative of activation to \n",
    "        \"\"\"\n",
    "#         print(type(z))\n",
    "#         print(\"z: \", z)\n",
    "#         print(\"gz: \", self.g(z))\n",
    "\n",
    "#         self.g(z)[self.g(z)<=0] = 0\n",
    "#         self.g(z)[self.g(z)>0] = 1\n",
    "        \n",
    "#         return z\n",
    "\n",
    "#         print(z)\n",
    "#         print(\"Hey: \", (self.g(z) < 1).astype(int))\n",
    "\n",
    "\n",
    "#         (x > 0) * 1\n",
    "\n",
    "        return (self.g(z) <= 2).astype(int) \n",
    "    \n",
    "#         return 1. * (self.g(z) > 0)\n",
    "\n",
    "    \n",
    "    def gradC(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate gradient of cost function for squared-loss C(a,y) = (a-y)^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        take an feature vector and propagate through network \n",
    "        \n",
    "        :param x: input feature vector \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: Initialize activation on initial layer to x \n",
    "        self.a[0] = x\n",
    "        \n",
    "        # TODO: Loop over layers and compute activities and activations \n",
    "        for ll in range(self.L - 1):\n",
    "            self.z[ll + 1] = np.dot(self.W[ll], self.a[ll]) + self.b[ll]\n",
    "            self.a[ll + 1] = self.g(self.z[ll + 1])\n",
    "            \n",
    "#         self.a = self.a\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts on the the data in X. Assume at least two output neurons so predictions\n",
    "        are one-hot encoded vectorized labels. \n",
    "        \n",
    "        :param X: a matrix of data to make predictions on \n",
    "        :return y: a matrix of vectorized labels \n",
    "        \"\"\"\n",
    "       # print(\"X: \", X)\n",
    "        yhat = np.zeros((X.shape[0], self.sizes[-1]), dtype=int)\n",
    "#         print(\"yhat: \", yhat)\n",
    "        \n",
    "        # TODO: Populate yhat with one-hot-coded predictions \n",
    "        for i, row in enumerate(X):\n",
    "            self.forward_prop(row)\n",
    "            \n",
    "            m = max(self.a[-1])\n",
    "            # anywhere in the last column of a equal the maximum element is 1, the rest of 0\n",
    "            b = np.where(self.a[-1] == m, 1, 0)\n",
    "            yhat[i] = b\n",
    "        \n",
    "        return yhat \n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        compute accuracy on labeled training set \n",
    "\n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vectorized true labels \n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        return np.sum(np.all(np.equal(yhat, y), axis=1)) / X.shape[0]\n",
    "            \n",
    "            \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation to get derivatives of C wrt weights and biases for given training example\n",
    "        \n",
    "        :param x: training features  \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: forward prop training example to fill in activities and activations \n",
    "        self.forward_prop(x)\n",
    "        \n",
    "        # TODO: compute deltas on output layer \n",
    "        self.delta[-1] = self.gradC(self.a[-1], y) * self.g_prime(self.z[-1])\n",
    "        \n",
    "        # TODO: loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "        for ll in range(self.L - 2, -1, -1): #loop backward\n",
    "            self.dW[ll] = np.outer(self.delta[ll + 1], self.a[ll])\n",
    "            self.db[ll] = self.delta[ll + 1]\n",
    "            self.delta[ll] = np.dot(self.W[ll].transpose(), self.delta[ll + 1]) * self.g_prime(self.z[ll])\n",
    "            \n",
    "            \n",
    "    def train(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "        \"\"\"\n",
    "        Train the network with SGD \n",
    "        \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        :param eta: learning rate \n",
    "        :param lam: regularization strength \n",
    "        :param num_epochs: number of epochs to run \n",
    "        :param isPrint: flag indicating to print training progress or not \n",
    "        \"\"\"\n",
    "        \n",
    "        counter = 1\n",
    "        \n",
    "        # initialize shuffled indices \n",
    "        shuffled_inds = list(range(X_train.shape[0]))\n",
    "        \n",
    "        # loop over training epochs \n",
    "        for ep in range(num_epochs):\n",
    "            \n",
    "            # shuffle indices \n",
    "            np.random.shuffle(shuffled_inds)\n",
    "            \n",
    "            # loop over training examples \n",
    "            for ind in shuffled_inds:\n",
    "                \n",
    "                # TODO: back prop to get derivatives \n",
    "                self.back_prop(X_train[ind, :], y_train[ind, :])\n",
    "        \n",
    "                # TODO: update weights and biases \n",
    "#                 self.W = [Wll - eta * dWll for Wll, dWll in zip(self.W, self.dW )]\n",
    "\n",
    "                self.W = [Wll - (eta) * (dWll + Wll*lam) for Wll, dWll in zip(self.W, self.dW )]\n",
    "                \n",
    "#                 (sum(self.dW[0]**2 * lam/2))\n",
    "                \n",
    "                self.b = [bll - (eta) * dbll for bll, dbll in zip(self.b, self.db)]\n",
    "            #print(self.W)   \n",
    "    \n",
    "            # occasionally print accuracy\n",
    "            if isPrint and ((ep+1)%10)==1:\n",
    "                self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "                \n",
    "        # print final accuracy\n",
    "        if isPrint:\n",
    "            self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "                    \n",
    "    def epoch_report(self, ep, num_epochs, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\"\n",
    "        Print the accuracy for the given epoch on training and validation data \n",
    "        \n",
    "        :param ep: the current epoch \n",
    "        :param num_epochs: the total number of epochs\n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"epoch {:3d}/{:3d}: \".format(ep+1, num_epochs), end=\"\")\n",
    "        print(\"  train acc: {:8.3f}\".format(self.accuracy(X_train, y_train)), end=\"\")\n",
    "        if X_valid is not None: print(\"  valid acc: {:8.3f}\".format(self.accuracy(X_valid, y_valid)))\n",
    "        else: print(\"\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1/ 50:   train acc:    0.797  valid acc:    0.781\n",
      "epoch  11/ 50:   train acc:    0.667  valid acc:    0.642\n",
      "epoch  21/ 50:   train acc:    0.712  valid acc:    0.694\n",
      "epoch  31/ 50:   train acc:    0.691  valid acc:    0.682\n",
      "epoch  41/ 50:   train acc:    0.728  valid acc:    0.713\n",
      "epoch  50/ 50:   train acc:    0.730  valid acc:    0.717\n"
     ]
    }
   ],
   "source": [
    "# 441 inputs: 441 pixel, a single hidden layer of 30 neurons, 4 outputs\n",
    "ReluTrainNetwork = ReluNetwork([441, 30, 4]) \n",
    "\n",
    "eta = 0.01\n",
    "lam = 0.0\n",
    "num_epochs = 50    \n",
    "\n",
    "#(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "\n",
    "ReluTrainNetwork.train(X_train, y_train, X_valid, y_valid, eta = eta , num_epochs = num_epochs, isPrint=True)\n",
    "\n",
    "result = ReluTrainNetwork.accuracy(X_train, y_train)\n",
    "\n",
    "# print(\"Accuracy is \", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{It's working but Relu function does not seem work better than tanh function.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxNetwork:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network \n",
    "        \n",
    "        :param sizes: a list of the number of neurons in each layer \n",
    "        \"\"\"\n",
    "        # save the number of layers in the network \n",
    "        self.L = len(sizes) \n",
    "        \n",
    "        # store the list of layer sizes \n",
    "        self.sizes = sizes  \n",
    "        \n",
    "        # initialize the bias vectors for each hidden and output layer \n",
    "        self.b = [np.random.randn(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the matrices of weights for each hidden and output layer \n",
    "        self.W = [np.random.randn(n, m) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the derivatives of biases for backprop \n",
    "        self.db = [np.zeros(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the derivatives of weights for backprop \n",
    "        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the activities on each hidden and output layer \n",
    "        self.z = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the activations on each hidden and output layer \n",
    "        self.a = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the deltas on each hidden and output layer \n",
    "        self.delta = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        self.train_accuracy = []\n",
    "        self.valid_accuracy = []\n",
    "        \n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply activation to \n",
    "        \"\"\"\n",
    "        z = np.clip(z, -20, 20)\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply derivative of activation to \n",
    "        \"\"\"\n",
    "        return self.g(z) * (1.0 - self.g(z))\n",
    "    \n",
    "    def gradC(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate gradient of cost function for squared-loss C(a,y) = (a-y)^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return - np.multiply(y, np.log(a)).sum() / a.shape[0]\n",
    "#         exps = np.exp(a - np.max(a))\n",
    "#         return exps / np.sum(exps)\n",
    "\n",
    "# get_cost(self, Y, T):\n",
    "#         \"\"\"Return the cost at the output of this output layer.\"\"\"\n",
    "#         return - np.multiply(T, np.log(Y)).sum() / Y.shape[0]\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        take an feature vector and propagate through network \n",
    "        \n",
    "        :param x: input feature vector \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: Initialize activation on initial layer to x \n",
    "        self.a[0] = x\n",
    "        \n",
    "        # TODO: Loop over layers and compute activities and activations \n",
    "        for ll in range(self.L - 1):\n",
    "            self.z[ll + 1] = np.dot(self.W[ll], self.a[ll]) + self.b[ll]\n",
    "            self.a[ll + 1] = self.g(self.z[ll + 1])\n",
    "            \n",
    "#         self.a = self.a\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts on the the data in X. Assume at least two output neurons so predictions\n",
    "        are one-hot encoded vectorized labels. \n",
    "        \n",
    "        :param X: a matrix of data to make predictions on \n",
    "        :return y: a matrix of vectorized labels \n",
    "        \"\"\"\n",
    "       # print(\"X: \", X)\n",
    "        yhat = np.zeros((X.shape[0], self.sizes[-1]), dtype=int)\n",
    "#         print(\"yhat: \", yhat)\n",
    "        \n",
    "        # TODO: Populate yhat with one-hot-coded predictions \n",
    "        for i, row in enumerate(X):\n",
    "            \n",
    "#             for ii in range(self.L - 1):\n",
    "#                 z = np.dot(self.W[ll], a) + self.b[ll]\n",
    "#                 a = self.g(z)\n",
    "#             yaht[i][np.argmax(a)]\n",
    "\n",
    "            self.forward_prop(row)\n",
    "            \n",
    "            m = max(self.a[-1])\n",
    "            # anywhere in the last column of a equal the maximum element is 1, the rest of 0\n",
    "            b = np.where(self.a[-1] == m, 1, 0)\n",
    "            yhat[i] = b\n",
    "        \n",
    "        return yhat \n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        compute accuracy on labeled training set \n",
    "\n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vectorized true labels \n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        return np.sum(np.all(np.equal(yhat, y), axis=1)) / X.shape[0]\n",
    "            \n",
    "            \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation to get derivatives of C wrt weights and biases for given training example\n",
    "        \n",
    "        :param x: training features  \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: forward prop training example to fill in activities and activations \n",
    "        self.forward_prop(x)\n",
    "        \n",
    "        # TODO: compute deltas on output layer \n",
    "        self.delta[-1] = self.gradC(self.a[-1], y) * self.g_prime(self.z[-1])\n",
    "        \n",
    "        # TODO: loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "        for ll in range(self.L - 2, -1, -1): #loop backward\n",
    "            self.dW[ll] = np.outer(self.delta[ll + 1], self.a[ll])\n",
    "            self.db[ll] = self.delta[ll + 1]\n",
    "            self.delta[ll] = np.dot(self.W[ll].transpose(), self.delta[ll + 1]) * self.g_prime(self.z[ll])\n",
    "            \n",
    "            \n",
    "    def train(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "        \"\"\"\n",
    "        Train the network with SGD \n",
    "        \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        :param eta: learning rate \n",
    "        :param lam: regularization strength \n",
    "        :param num_epochs: number of epochs to run \n",
    "        :param isPrint: flag indicating to print training progress or not \n",
    "        \"\"\"\n",
    "        \n",
    "        counter = 1\n",
    "        \n",
    "        # initialize shuffled indices \n",
    "        shuffled_inds = list(range(X_train.shape[0]))\n",
    "        \n",
    "        # loop over training epochs \n",
    "        for ep in range(num_epochs):\n",
    "            \n",
    "            # shuffle indices \n",
    "            np.random.shuffle(shuffled_inds)\n",
    "            \n",
    "            # loop over training examples \n",
    "            for ind in shuffled_inds:\n",
    "                \n",
    "                # TODO: back prop to get derivatives \n",
    "                self.back_prop(X_train[ind, :], y_train[ind, :])\n",
    "        \n",
    "                # TODO: update weights and biases \n",
    "#                 self.W = [Wll - eta * dWll for Wll, dWll in zip(self.W, self.dW )]\n",
    "\n",
    "                self.W = [Wll - eta * (dWll + Wll*lam) for Wll, dWll in zip(self.W, self.dW )]\n",
    "                \n",
    "#                 (sum(self.dW[0]**2 * lam/2))\n",
    "                \n",
    "                self.b = [bll - eta * dbll for bll, dbll in zip(self.b, self.db)]\n",
    "            #print(self.W)\n",
    "            \n",
    "            # occasionally print accuracy\n",
    "            if isPrint and ((ep+1)%5)==1:\n",
    "                self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "                \n",
    "        # print final accuracy\n",
    "        if isPrint:\n",
    "            self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "                    \n",
    "    def epoch_report(self, ep, num_epochs, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\"\n",
    "        Print the accuracy for the given epoch on training and validation data \n",
    "        \n",
    "        :param ep: the current epoch \n",
    "        :param num_epochs: the total number of epochs\n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"epoch {:3d}/{:3d}: \".format(ep+1, num_epochs), end=\"\")\n",
    "        print(\"  train acc: {:8.3f}\".format(self.accuracy(X_train, y_train)), end=\"\")\n",
    "        \n",
    "        # This part is for Problem 4\n",
    "        self.train_accuracy.append(self.accuracy(X_train, y_train))\n",
    "        if X_valid is not None: print(\"  valid acc: {:8.3f}\".format(self.accuracy(X_valid, y_valid)))\n",
    "        else: print(\"\")   \n",
    "        if X_valid is not None:\n",
    "            self.valid_accuracy.append(self.accuracy(X_valid, y_valid))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1/ 50:   train acc:    0.101  valid acc:    0.094\n",
      "epoch  11/ 50:   train acc:    0.811  valid acc:    0.801\n",
      "epoch  21/ 50:   train acc:    0.826  valid acc:    0.811\n",
      "epoch  31/ 50:   train acc:    0.806  valid acc:    0.778\n",
      "epoch  41/ 50:   train acc:    0.837  valid acc:    0.810\n",
      "epoch  50/ 50:   train acc:    0.845  valid acc:    0.816\n"
     ]
    }
   ],
   "source": [
    "# 441 inputs: 441 pixel, a single hidden layer of 30 neurons, 4 outputs\n",
    "ReluTrainNetwork = ReluNetwork([441, 30, 4]) \n",
    "\n",
    "eta = 0.01\n",
    "lam = 0.0\n",
    "num_epochs = 50    \n",
    "\n",
    "#(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "\n",
    "ReluTrainNetwork.train(X_train, y_train, X_valid, y_valid, eta = eta , num_epochs = num_epochs, isPrint=True)\n",
    "\n",
    "result = ReluTrainNetwork.accuracy(X_train, y_train)\n",
    "\n",
    "# print(\"Accuracy is \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Softmax does not seem work better than tanh too}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
