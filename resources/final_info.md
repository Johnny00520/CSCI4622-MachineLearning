Final Exam Information 
=

**Time**/**Date**: 9. May from **1:30-4pm**  in **ECCR 200**

Overview and Rules  
--------
- The exam covers everything we have done in class up to and including Boosting.  This includes all material presented in lecture, in-class discussion, Hands-On notebooks, and material introduced in homework. 
- You are allowed **TWO** 8-1/2 x 11in sheet of **handwritten** notes (both sides).  No magnifying glasses! 
- You may use a calculator provided that it cannot access the internet or store large amounts of data. 
- The exam will be a mixture of multiple choice questions and free-response questions in which you may be asked to work through simple examples of algorithms
- The final exam will be cumulative, but will emphasize material not covered on the midterm exam 


Material Overview 
---

**General**
- differences between classification and regression 
- differences between supervised and unsupervised learning 

**Regression**
- probabilistic interpretation of regression models 
- generalities of how weights are estimated 
- interpretation of the weights in regression models 

**Regularization** 
- the point of regularization 
- Ridge (L2) Regression 
- the effect on bias/variance 

**Learning Theory**
- the Bias/Variance Trade-Off
- overfitting, generalization, etc 

**K-Nearest Neighbors**
- how the algorithm works 
- how to perform classification with the algorithm 
- properties of the algorithm 

**The Perceptron**
- assumptions behind the model 
- how the model makes predictions 
- how the model is trained 
- theoretical guarantees for convergence 

**Logistic Regression**
- assumptions behind the algorithm 
- it's probabilistic interpretation 
- how to perform classification with the algorithm 

**Stochastic Gradient Descent**
- Maximum Likelihood Estimation 
- how to find the weights in Regression 
- how to find the weights in Logistic Regression 

**Basic Text Models**
- binary text features 
- bag-of-words models 
- idea behind TF-IDF 

**Neural Networks**
- the multilayer perceptron 
- feed-forward neural networks
- different activation functions 
- back propagation and SGD for learning weights

**Validation and Evaluation**
- cross-validation 
- learning curves 
- ROC Curves and AUC 

**Naive Bayes**
- discrete Naive Bayes 
- add-one smoothing 
- continuous Naive Bayes 

**VC Dimension**
- determining VC dimension of a given hypothesis class
- VC dimension for general linear classifiers 

**Support Vector Machines**
- Hard-Margin SVM 
- Soft-Margin SVM 
- the Kernel Trick 

**Multiclass Classification**
- one-vs-all 
- all-pairs 
- error-correcting output code schemes 

**Decision Trees and Ensemble Methods**
- impurity measures and information gain  
- determining best splits 
- bagging and Random Forests 
- AdaBoost 
















